{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Chatbot IA para WhatsApp (EvolutionAPI + LangChain)","text":"<p>Este reposit\u00f3rio cont\u00e9m um chatbot para WhatsApp que integra:</p> <ul> <li>EvolutionAPI (painel + gateway para WhatsApp)</li> <li>FastAPI para expor o webhook e endpoints</li> <li>LangChain (Fluxos RAG) para recuperar documentos e conversar com LLMs</li> <li>Chroma para persist\u00eancia de vetores</li> <li>Modelos Groq (ChatGroq) para LLM e FastEmbed/BGE para embeddings</li> <li>Docker / Docker Compose para facilitar deploy</li> </ul> <p>Este README explica como configurar, executar e ajustar o projeto.</p>"},{"location":"#sumario","title":"Sum\u00e1rio","text":"<ul> <li>Pr\u00e9-requisitos</li> <li>Vari\u00e1veis de ambiente (exemplo)</li> <li>Como executar (Docker e local)</li> <li>Adicionar documentos para RAG (vetoriza\u00e7\u00e3o)</li> <li>Qual modelo usar e recomenda\u00e7\u00f5es</li> <li>Troubleshooting e notas</li> </ul>"},{"location":"#1-pre-requisitos","title":"1) Pr\u00e9-requisitos","text":"<ul> <li>Docker &amp; Docker Compose (recomendado para produ\u00e7\u00e3o/testes locais consistentes)</li> <li>Python 3.11+ (para execu\u00e7\u00e3o local)</li> <li>Chaves/credenciais da EvolutionAPI</li> <li>(Opcional) Conta/credenciais para provedores de modelos se for usar outros LLMs</li> </ul>"},{"location":"#2-variaveis-de-ambiente","title":"2) Vari\u00e1veis de ambiente","text":"<p>Crie um arquivo <code>.env</code> na raiz do projeto. Abaixo est\u00e1 um exemplo com vari\u00e1veis detectadas no c\u00f3digo \u2014 ajuste conforme necess\u00e1rio:</p> <pre><code># EvolutionAPI\nEVOLUTION_API_URL=http://localhost:8080/api\nEVOLUTION_INSTANCE_NAME=nome_da_instancia\nAUTHENTICATION_API_KEY=suachave_da_evolution\n\n# Groq (LLM) - usado pelo ChatGroq\nGROQ_MODEL_NAME=llama-3.3-70b-versatile (exemplo ou outro modelo Groq compat\u00edvel)\nGROQ_MODEL_TEMPERATURE=0\n\n# Vetor store / RAG\nRAG_FILES_DIR=./rag_files\nVECTOR_STORE_PATH=./vectorstore_data\n\n# Redis (cache) \u2014 usado pelo docker-compose como `CACHE_REDIS_URI`\nCACHE_REDIS_URI=redis://redis:6379/0\n\n# Opcional: configura prompts via vari\u00e1veis\nAI_CONTEXTUALIZE_PROMPT=\"Seu prompt de contextualiza\u00e7\u00e3o aqui\"\nAI_SYSTEM_PROMPT=\"Seu prompt de sistema (papel do assistente) aqui\"\n</code></pre> <p>Observa\u00e7\u00f5es: - <code>EVOLUTION_INSTANCE_NAME</code> deve corresponder ao nome da inst\u00e2ncia configurada no painel da EvolutionAPI (Manager). - <code>EVOLUTION_API_URL</code> aponta para onde o servi\u00e7o EvolutionAPI est\u00e1 escutando (no docker-compose padr\u00e3o: http://localhost:8080/api). - <code>GROQ_MODEL_NAME</code> e <code>GROQ_MODEL_TEMPERATURE</code> controlam qual modelo Groq ser\u00e1 usado pelo <code>langchain_groq.ChatGroq</code>. - <code>RAG_FILES_DIR</code> \u00e9 a pasta onde voc\u00ea coloca PDFs e .txt para vetoriza\u00e7\u00e3o.</p>"},{"location":"#3-como-executar","title":"3) Como executar","text":"<p>3.1 Usando Docker Compose (recomendado)</p> <ol> <li>Copie/exemplo e edite <code>.env</code>:</li> </ol> <pre><code>cp .env.example .env || true\n# edite .env com suas chaves\n</code></pre> <ol> <li>Suba os servi\u00e7os:</li> </ol> <pre><code>docker-compose up --build\n</code></pre> <p>Servi\u00e7os principais: - <code>evolution-api</code> \u2014 painel e gateway para WhatsApp - <code>bot</code> \u2014 aplica\u00e7\u00e3o FastAPI que exp\u00f5e <code>/webhook</code> - <code>redis</code>, <code>postgres</code> \u2014 depend\u00eancias do EvolutionAPI</p> <p>Depois que o painel do EvolutionAPI estiver no ar, adicione sua inst\u00e2ncia do WhatsApp no painel (Manager) e configure o webhook da inst\u00e2ncia para:</p> <pre><code>http://bot:8000/webhook\n</code></pre> <p>Habilite o evento <code>MESSAGES_UPSERT</code> para receber mensagens.</p> <p>3.2 Execu\u00e7\u00e3o local (sem Docker)</p> <ol> <li>Crie e ative um virtualenv, instale depend\u00eancias:</li> </ol> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\n</code></pre> <ol> <li>Exporte as vari\u00e1veis de ambiente (ou use <code>.env</code> com python-dotenv) e rode o servidor:</li> </ol> <pre><code># com uvicorn\nuvicorn app:app --host 0.0.0.0 --port 8000\n</code></pre> <p>Nota: se executar localmente, voc\u00ea ainda precisa de um EvolutionAPI acess\u00edvel (pode rodar via docker-compose apenas o servi\u00e7o <code>evolution-api</code>) e apontar <code>EVOLUTION_API_URL</code> para ele.</p>"},{"location":"#4-adicionar-documentos-para-rag","title":"4) Adicionar documentos para RAG","text":"<ol> <li>Coloque arquivos PDF ou TXT dentro da pasta <code>rag_files/</code> na raiz do projeto.</li> <li>No pr\u00f3ximo start do servi\u00e7o <code>bot</code> (ou ao chamar o c\u00f3digo manualmente), o m\u00f3dulo <code>vectorstore.py</code>:<ul> <li>vai carregar os documentos (PyPDFLoader / TextLoader),</li> <li>dividir em chunks (chunk_size=1000, overlap=200),</li> <li>gerar embeddings com <code>FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")</code>,</li> <li>persistir no diret\u00f3rio indicado por <code>VECTOR_STORE_PATH</code> (ex: <code>./vectorstore_data</code>).</li> </ul> </li> </ol> <p>Os arquivos processados ser\u00e3o movidos para <code>rag_files/processed/</code>.</p> <p>Dica: para re-processar todos os documentos, remova ou renomeie a pasta <code>rag_files/processed</code> para que os arquivos voltem a ser lidos.</p>"},{"location":"#5-modelos-e-recomendacoes","title":"5) Modelos e recomenda\u00e7\u00f5es","text":"<ul> <li>LLM principal no projeto: ChatGroq (via <code>langchain_groq.ChatGroq</code>). A vari\u00e1vel <code>GROQ_MODEL_NAME</code> define qual modelo Groq usar. Exemplos: <code>openai/gpt-oss-20b</code> (verifique disponibilidade/compatibilidade com sua conta Groq).</li> <li>Embeddings: <code>BAAI/bge-base-en-v1.5</code> via <code>FastEmbedEmbeddings</code> (bom trade-off entre custo e qualidade para embeddings).</li> <li>Vetor store: Chroma (persist\u00eancia local em <code>VECTOR_STORE_PATH</code>).</li> </ul> <p>Sugest\u00f5es: - Para conversas em portugu\u00eas, teste a qualidade do modelo e ajuste <code>GROQ_MODEL_TEMPERATURE</code> (0.0-0.5 para respostas mais determin\u00edsticas). - Se quiser usar OpenAI ou outro provedor, ser\u00e1 necess\u00e1rio adaptar <code>chains.py</code> para criar o LLM correspondente (por exemplo, <code>OpenAI</code> ou <code>ChatOpenAI</code>) e ajustar vari\u00e1veis de ambiente.</p>"},{"location":"#6-estrutura-importante-de-arquivos","title":"6) Estrutura importante de arquivos","text":"<ul> <li><code>app.py</code> \u2014 entrada FastAPI e webhook <code>/webhook</code> que processa eventos da EvolutionAPI.</li> <li><code>chains.py</code> \u2014 constr\u00f3i o chain RAG com hist\u00f3rico, ChatGroq e retrieval.</li> <li><code>vectorstore.py</code> \u2014 carrega documentos, cria/salva Chroma e embeddings.</li> <li><code>evolution_api.py</code> \u2014 fun\u00e7\u00e3o <code>send_whatsapp_message</code> que chama a API Evolution para enviar respostas.</li> <li><code>prompts.py</code> \u2014 prompts usados nos fluxos (carregados de <code>AI_CONTEXTUALIZE_PROMPT</code> e <code>AI_SYSTEM_PROMPT</code>).</li> </ul>"},{"location":"#7-troubleshooting-notas","title":"7) Troubleshooting / notas","text":"<ul> <li>Webhook 404/500: verifique se o container <code>bot</code> est\u00e1 rodando e exp\u00f5e a porta 8000. Verifique logs do container:</li> </ul> <pre><code>docker-compose logs -f bot\n</code></pre> <ul> <li>Mensagens n\u00e3o chegam no bot: confirme que o webhook configurado na inst\u00e2ncia do EvolutionAPI est\u00e1 exatamente <code>http://bot:8000/webhook</code> (ou <code>http://localhost:8000/webhook</code> se exposto diretamente) e que <code>MESSAGES_UPSERT</code> est\u00e1 habilitado.</li> <li>Vetores n\u00e3o s\u00e3o encontrados / base vazia: verifique <code>VECTOR_STORE_PATH</code> e se o processo de vetoriza\u00e7\u00e3o foi executado. Logs ao iniciar o bot mostrar\u00e3o a cria\u00e7\u00e3o/persist\u00eancia do Chroma.</li> <li>Problemas de credenciais: confirme <code>AUTHENTICATION_API_KEY</code> e <code>EVOLUTION_INSTANCE_NAME</code> no painel da EvolutionAPI.</li> </ul>"},{"location":"#8-como-contribuir","title":"8) Como contribuir","text":"<ul> <li>Abra issues descrevendo bugs ou melhorias.</li> <li>Envie PRs com testes pequenos e documenta\u00e7\u00e3o.</li> </ul>"},{"location":"#9-resumo-rapido-cheat-sheet","title":"9) Resumo r\u00e1pido (cheat sheet)","text":"<ul> <li>Subir tudo com Docker:</li> </ul> <pre><code>cp .env.example .env\n# editar .env\ndocker-compose up --build\n</code></pre> <ul> <li>Executar localmente:</li> </ul> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\nuvicorn app:app --host 0.0.0.0 --port 8000\n</code></pre>"},{"location":"app/","title":"App","text":""},{"location":"app/#app","title":"<code>app</code>","text":"<p>Aplica\u00e7\u00e3o FastAPI que exp\u00f5e o endpoint de webhook usado pela EvolutionAPI.</p> <p>Este m\u00f3dulo inicializa a cadeia conversacional RAG e trata eventos de webhook recebidos. Extrai os campos relevantes da mensagem, envia a entrada ao fluxo RAG para gerar a resposta e encaminha a resposta de volta via EvolutionAPI.</p>"},{"location":"app/#app.webhook","title":"<code>webhook(request)</code>  <code>async</code>","text":"<p>Recebe e processa eventos de webhook enviados pela EvolutionAPI.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>requisi\u00e7\u00e3o contendo o JSON do webhook.</p> required Comportamento <ul> <li>Extrai de forma defensiva os campos <code>remoteJid</code>, <code>fromMe</code>,   <code>conversation</code> e <code>pushName</code> do payload.</li> <li>Registra em log apenas os campos solicitados (remoteJid, pushName,   conversation).</li> <li>Quando aplic\u00e1vel, envia o texto ao chain RAG e encaminha a   resposta via EvolutionAPI.</li> </ul> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, object]</code> <p>{\"status\": \"ok\"} indicando que o webhook foi processado.</p> Source code in <code>app.py</code> <pre><code>@app.post(\"/webhook\")\nasync def webhook(request: Request) -&gt; Dict[str, object]:\n    \"\"\"Recebe e processa eventos de webhook enviados pela EvolutionAPI.\n\n    Args:\n        request (fastapi.Request): requisi\u00e7\u00e3o contendo o JSON do webhook.\n\n    Comportamento:\n        - Extrai de forma defensiva os campos `remoteJid`, `fromMe`,\n          `conversation` e `pushName` do payload.\n        - Registra em log apenas os campos solicitados (remoteJid, pushName,\n          conversation).\n        - Quando aplic\u00e1vel, envia o texto ao chain RAG e encaminha a\n          resposta via EvolutionAPI.\n\n    Returns:\n        dict: {\"status\": \"ok\"} indicando que o webhook foi processado.\n    \"\"\"\n\n    data: Dict[str, object] = await request.json()\n    # estrutura esperada do webhook: data.data.key.remoteJid, data.data.key.fromMe, data.data.message.conversation\n    chat_id: Optional[str] = data.get(\"data\").get(\"key\").get(\"remoteJid\")\n    from_me: Optional[bool] = data.get(\"data\").get(\"key\").get(\"fromMe\")\n    message: Optional[str] = (\n        data.get(\"data\").get(\"message\").get(\"conversation\")\n    )\n\n    logger.info(\n        \"Webhook recebido - remoteJid={}\",\n        chat_id,\n    )\n\n    if chat_id and message and from_me and \"@g.us\" not in chat_id:\n        await buffer_message(chat_id=chat_id, message=message)\n\n    return {\"status\": \"ok\"}\n</code></pre>"},{"location":"app/#config","title":"<code>config</code>","text":"<p>Carrega e exp\u00f5e as vari\u00e1veis de ambiente usadas pelo projeto.</p> <p>As vari\u00e1veis s\u00e3o carregadas via python-dotenv (<code>.env</code>) e exportadas como vari\u00e1veis de m\u00f3dulo para uso em outros pontos do c\u00f3digo.</p>"},{"location":"app/#message_buffer","title":"<code>message_buffer</code>","text":"<p>M\u00f3dulo para fazer o buffer de mensagens recebidas via WhatsApp usando Redis com debounce.</p>"},{"location":"app/#message_buffer.buffer_message","title":"<code>buffer_message(chat_id, message)</code>  <code>async</code>","text":"<p>Armazena mensagens em buffer no Redis com debounce.</p> <p>Parameters:</p> Name Type Description Default <code>chat_id</code> <code>str</code> <p>ID do chat (remoteJid).</p> required <code>message</code> <code>str</code> <p>Mensagem recebida.</p> required Source code in <code>message_buffer.py</code> <pre><code>async def buffer_message(chat_id: str, message: str) -&gt; None:\n    \"\"\"Armazena mensagens em buffer no Redis com debounce.\n\n    Args:\n        chat_id (str): ID do chat (remoteJid).\n        message (str): Mensagem recebida.\n    \"\"\"\n\n    buffer_key = f\"{chat_id}{BUFFER_KEY_SUFFIX}\"\n\n    await redis_client.rpush(buffer_key, message)\n    await redis_client.expire(buffer_key, int(BUFFER_TTL))\n\n    logger.info(\"Mensagem adicionada ao buffer para o chat_id {}.\", chat_id)\n\n    if debounce_tasks.get(chat_id):\n        debounce_tasks[chat_id].cancel()\n        logger.info(\"Debounce resetado para o chat_id {}.\", chat_id)\n\n    debounce_tasks[chat_id] = asyncio.create_task(handle_buffered(chat_id))\n</code></pre>"},{"location":"app/#message_buffer.handle_buffered","title":"<code>handle_buffered(chat_id)</code>  <code>async</code>","text":"<p>Processa mensagens em buffer ap\u00f3s o per\u00edodo de debounce.</p> <p>Parameters:</p> Name Type Description Default <code>chat_id</code> <code>str</code> <p>ID do chat (remoteJid).</p> required Source code in <code>message_buffer.py</code> <pre><code>async def handle_buffered(chat_id: str) -&gt; None:\n    \"\"\"Processa mensagens em buffer ap\u00f3s o per\u00edodo de debounce.\n\n    Args:\n        chat_id (str): ID do chat (remoteJid).\n    \"\"\"\n\n    try:\n        logger.info(\"Iniciando debounce para o chat_id {}.\", chat_id)\n        await asyncio.sleep(float(DEBOUNCE_SECONDS))\n\n        buffer_key = f\"{chat_id}{BUFFER_KEY_SUFFIX}\"\n        messages = await redis_client.lrange(buffer_key, 0, -1)\n\n        full_message = \" \".join(messages).strip()\n\n        if full_message:\n            logger.info(\n                \"Enviando mensagem agrupada para o chat_id {}: {}.\",\n                chat_id,\n                full_message,\n            )\n\n            ai_response: str = conversational_rag_chain.invoke(\n                input={\"input\": full_message},\n                config={\"configurable\": {\"session_id\": chat_id}},\n            )[\"answer\"]\n\n            send_whatsapp_message(\n                number=chat_id,\n                text=ai_response,\n            )\n            logger.info(\"Resposta enviada para {}\", chat_id)\n\n        await redis_client.delete(buffer_key)\n\n    except asyncio.CancelledError:\n        logger.info(\"Debounce cancelado para o chat_id {}.\", chat_id)\n</code></pre>"},{"location":"app/#chains","title":"<code>chains</code>","text":"<p>Constru\u00e7\u00e3o dos chains RAG usados pelo bot.</p> <p>Este m\u00f3dulo cria o retriever (Chroma) e combina com um LLM (ChatGroq) para montar o fluxo de RAG que responde \u00e0s consultas do usu\u00e1rio, preservando o hist\u00f3rico de mensagens por sess\u00e3o.</p>"},{"location":"app/#chains.get_conversational_rag_chain","title":"<code>get_conversational_rag_chain()</code>","text":"<p>Retorna um RunnableWithMessageHistory que envolve o chain RAG.</p> <p>Esse wrapper preserva e injeta o hist\u00f3rico de mensagens por sess\u00e3o quando o chain \u00e9 executado, permitindo conversas stateful por <code>session_id</code>.</p> <p>Returns:</p> Name Type Description <code>RunnableWithMessageHistory</code> <code>RunnableWithMessageHistory</code> <p>runnable que gerencia hist\u00f3rico por sess\u00e3o.</p> Source code in <code>chains.py</code> <pre><code>def get_conversational_rag_chain() -&gt; RunnableWithMessageHistory:\n    \"\"\"Retorna um RunnableWithMessageHistory que envolve o chain RAG.\n\n    Esse wrapper preserva e injeta o hist\u00f3rico de mensagens por sess\u00e3o quando\n    o chain \u00e9 executado, permitindo conversas stateful por `session_id`.\n\n    Returns:\n        RunnableWithMessageHistory: runnable que gerencia hist\u00f3rico por sess\u00e3o.\n    \"\"\"\n\n    rag_chain: object = get_rag_chain()\n    return RunnableWithMessageHistory(\n        runnable=rag_chain,\n        get_session_history=get_session_history,\n        input_messages_key=\"input\",\n        history_messages_key=\"chat_history\",\n        output_messages_key=\"answer\",\n    )\n</code></pre>"},{"location":"app/#chains.get_rag_chain","title":"<code>get_rag_chain()</code>","text":"<p>Cria e retorna o chain b\u00e1sico de RAG.</p> <p>O chain combina o LLM ChatGroq com um retriever (Chroma) e dois componentes principais: um history-aware retriever e uma cadeia de resposta (stuff documents).</p> <p>Returns:</p> Name Type Description <code>object</code> <code>object</code> <p>Chain pronto para ser usado pelo fluxo de recupera\u00e7\u00e3o + QA.</p> Source code in <code>chains.py</code> <pre><code>def get_rag_chain() -&gt; object:\n    \"\"\"Cria e retorna o chain b\u00e1sico de RAG.\n\n    O chain combina o LLM ChatGroq com um retriever (Chroma) e dois\n    componentes principais: um history-aware retriever e uma cadeia de\n    resposta (stuff documents).\n\n    Returns:\n        object: Chain pronto para ser usado pelo fluxo de recupera\u00e7\u00e3o + QA.\n    \"\"\"\n\n    llm = ChatGroq(\n        model=GROQ_MODEL_NAME,\n        temperature=GROQ_MODEL_TEMPERATURE,\n    )\n    retriever = get_vectorstore().as_retriever()\n\n    history_aware_chain = create_history_aware_retriever(\n        llm, retriever, contextualize_prompt\n    )\n    question_answer_chain = create_stuff_documents_chain(\n        llm=llm,\n        prompt=qa_prompt,\n    )\n    chain = create_retrieval_chain(history_aware_chain, question_answer_chain)\n    logger.info(\"RAG chain criado com LLM={}\", GROQ_MODEL_NAME)\n    return chain\n</code></pre>"},{"location":"app/#vectorstore","title":"<code>vectorstore</code>","text":"<p>Carregamento de documentos, prepara\u00e7\u00e3o e inicializa\u00e7\u00e3o do Chroma.</p> <p>Este m\u00f3dulo procura arquivos em <code>RAG_FILES_DIR</code>, carrega PDFs e textos, gera splits e persiste/recupera o Chroma vectorstore usando FastEmbedEmbeddings.</p>"},{"location":"app/#vectorstore.get_vectorstore","title":"<code>get_vectorstore()</code>","text":"<p>Inicializa ou carrega o Chroma vectorstore.</p> <ul> <li>Se existirem documentos novos em <code>RAG_FILES_DIR</code>, carrega-os e cria   embeddings, persistindo-os em <code>VECTOR_STORE_PATH</code>.</li> <li>Caso contr\u00e1rio, inicializa um Chroma vazio apontando para   <code>VECTOR_STORE_PATH</code>.</li> </ul> <p>Returns:</p> Name Type Description <code>Chroma</code> <code>Chroma</code> <p>inst\u00e2ncia do vectorstore pronta para uso.</p> Source code in <code>vectorstore.py</code> <pre><code>def get_vectorstore() -&gt; Chroma:\n    \"\"\"Inicializa ou carrega o Chroma vectorstore.\n\n    - Se existirem documentos novos em `RAG_FILES_DIR`, carrega-os e cria\n      embeddings, persistindo-os em `VECTOR_STORE_PATH`.\n    - Caso contr\u00e1rio, inicializa um Chroma vazio apontando para\n      `VECTOR_STORE_PATH`.\n\n    Returns:\n        Chroma: inst\u00e2ncia do vectorstore pronta para uso.\n    \"\"\"\n\n    docs: List[DocumentProtocol] = load_documents()\n    logger.info(\"N\u00famero de documentos carregados: {}\", len(docs))\n    if docs:\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n        )\n\n        splits = text_splitter.split_documents(docs)\n        chroma = Chroma.from_documents(\n            documents=splits,\n            embedding=embedding,\n            persist_directory=VECTOR_STORE_PATH,\n        )\n        logger.info(\"Chroma persistido em {}\", VECTOR_STORE_PATH)\n        return chroma\n\n    logger.info(\"Inicializando Chroma vazio em {}\", VECTOR_STORE_PATH)\n    return Chroma(\n        embedding_function=embedding,\n        persist_directory=VECTOR_STORE_PATH,\n    )\n</code></pre>"},{"location":"app/#vectorstore.load_documents","title":"<code>load_documents()</code>","text":"<p>Carrega documentos a partir de <code>RAG_FILES_DIR</code>.</p> <ul> <li>Procura por arquivos <code>.pdf</code> e <code>.txt</code> em <code>RAG_FILES_DIR</code>.</li> <li>Carrega o conte\u00fado em objetos compat\u00edveis com DocumentProtocol.</li> <li>Move os arquivos processados para a subpasta <code>processed</code>.</li> </ul> <p>Returns:</p> Type Description <code>List[DocumentProtocol]</code> <p>List[DocumentProtocol]: lista de documentos carregados.</p> Source code in <code>vectorstore.py</code> <pre><code>def load_documents() -&gt; List[DocumentProtocol]:\n    \"\"\"Carrega documentos a partir de `RAG_FILES_DIR`.\n\n    - Procura por arquivos `.pdf` e `.txt` em `RAG_FILES_DIR`.\n    - Carrega o conte\u00fado em objetos compat\u00edveis com DocumentProtocol.\n    - Move os arquivos processados para a subpasta `processed`.\n\n    Returns:\n        List[DocumentProtocol]: lista de documentos carregados.\n    \"\"\"\n\n    docs: List[DocumentProtocol] = []\n\n    processed_dir: str = os.path.join(RAG_FILES_DIR, \"processed\")\n    os.makedirs(processed_dir, exist_ok=True)\n    files = [\n        os.path.join(RAG_FILES_DIR, f)\n        for f in os.listdir(RAG_FILES_DIR)\n        if f.endswith((\".pdf\") or f.endswith(\".txt\"))\n    ]\n    logger.info(\"Arquivos para vetoriza\u00e7\u00e3o encontrados: {}\", files)\n\n    for file in files:\n        loader = (\n            PyPDFLoader(file) if file.endswith(\".pdf\") else TextLoader(file)\n        )\n        docs.extend(loader.load())\n        dest_path: str = os.path.join(processed_dir, os.path.basename(file))\n        shutil.move(file, dest_path)\n\n    return docs\n</code></pre>"},{"location":"app/#memory","title":"<code>memory</code>","text":""},{"location":"app/#memory.get_session_history","title":"<code>get_session_history(session_id)</code>","text":"<p>Retorna uma inst\u00e2ncia de RedisChatMessageHistory para o session_id dado.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>identificador da sess\u00e3o/hist\u00f3rico</p> required <p>Returns:</p> Name Type Description <code>RedisChatMessageHistory</code> <code>RedisChatMessageHistory</code> <p>objeto que gerencia o hist\u00f3rico no Redis</p> Source code in <code>memory.py</code> <pre><code>def get_session_history(session_id: str) -&gt; RedisChatMessageHistory:\n    \"\"\"Retorna uma inst\u00e2ncia de RedisChatMessageHistory para o session_id dado.\n\n    Args:\n        session_id: identificador da sess\u00e3o/hist\u00f3rico\n\n    Returns:\n        RedisChatMessageHistory: objeto que gerencia o hist\u00f3rico no Redis\n    \"\"\"\n    return RedisChatMessageHistory(\n        session_id=session_id,\n        url=REDIS_URL,\n    )\n</code></pre>"},{"location":"app/#evolution_api","title":"<code>evolution_api</code>","text":""},{"location":"app/#evolution_api.send_whatsapp_message","title":"<code>send_whatsapp_message(number, text)</code>","text":"<p>Envia uma mensagem de texto para um n\u00famero/ID via EvolutionAPI.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>str</code> <p>n\u00famero ou remoteJid (ex: '5511999999999@c.us' ou id de chat)</p> required <code>text</code> <code>str</code> <p>texto da mensagem a ser enviada</p> required Source code in <code>evolution_api.py</code> <pre><code>def send_whatsapp_message(number: str, text: str) -&gt; None:\n    \"\"\"Envia uma mensagem de texto para um n\u00famero/ID via EvolutionAPI.\n\n    Args:\n        number: n\u00famero ou remoteJid (ex: '5511999999999@c.us' ou id de chat)\n        text: texto da mensagem a ser enviada\n    \"\"\"\n    url: Optional[str] = (\n        f\"{EVOLUTION_API_URL}/message/sendText/{EVOLUTION_INSTANCE_NAME}\"\n        if EVOLUTION_API_URL and EVOLUTION_INSTANCE_NAME\n        else None\n    )\n    if not url:\n        logger.error(\n            \"EVOLUTION_API_URL ou EVOLUTION_INSTANCE_NAME n\u00e3o configurados\"\n        )\n        return\n\n    headers = {\n        \"apikey\": EVOLUTION_AUTHENTICATION_API_KEY,\n        \"Content-Type\": \"application/json\",\n    }\n    payload = {\"number\": number, \"text\": text}\n    try:\n        requests.post(url, json=payload, headers=headers)\n    except Exception as e:\n        logger.exception(\"Erro ao enviar mensagem via EvolutionAPI: {}\", e)\n</code></pre>"},{"location":"app/#prompts","title":"<code>prompts</code>","text":"<p>Templates de prompt usados para contextualiza\u00e7\u00e3o e QA.</p> <p>Este m\u00f3dulo define dois ChatPromptTemplate: um para contextualizar a pergunta com o hist\u00f3rico e outro para a etapa de QA (resposta usando o conte\u00fado recuperado).</p>"},{"location":"app/#memory","title":"<code>memory</code>","text":""},{"location":"app/#memory.get_session_history","title":"<code>get_session_history(session_id)</code>","text":"<p>Retorna uma inst\u00e2ncia de RedisChatMessageHistory para o session_id dado.</p> <p>Parameters:</p> Name Type Description Default <code>session_id</code> <code>str</code> <p>identificador da sess\u00e3o/hist\u00f3rico</p> required <p>Returns:</p> Name Type Description <code>RedisChatMessageHistory</code> <code>RedisChatMessageHistory</code> <p>objeto que gerencia o hist\u00f3rico no Redis</p> Source code in <code>memory.py</code> <pre><code>def get_session_history(session_id: str) -&gt; RedisChatMessageHistory:\n    \"\"\"Retorna uma inst\u00e2ncia de RedisChatMessageHistory para o session_id dado.\n\n    Args:\n        session_id: identificador da sess\u00e3o/hist\u00f3rico\n\n    Returns:\n        RedisChatMessageHistory: objeto que gerencia o hist\u00f3rico no Redis\n    \"\"\"\n    return RedisChatMessageHistory(\n        session_id=session_id,\n        url=REDIS_URL,\n    )\n</code></pre>"}]}